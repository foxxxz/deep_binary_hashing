{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:(50000, 32, 32, 3), y_train:(50000,)\n",
      "x_test:(10000, 32, 32, 3), y_test:(10000,)\n",
      "y_train_ohe: (50000, 10)\n",
      "y_test_ohe: (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "ohe = OneHotEncoder()\n",
    "y_train_ohe = ohe.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "y_test_ohe = ohe.transform(y_test.reshape(-1,1)).toarray().astype('float32')\n",
    "print('x_train:{}, y_train:{}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test:{}, y_test:{}'.format(x_test.shape, y_test.shape))\n",
    "print('y_train_ohe:', y_train_ohe.shape)\n",
    "print('y_test_ohe:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['seed'] = 0\n",
    "params['embedding'] = 256\n",
    "params['n_classes'] = 10\n",
    "params['labels'] = np.unique(y_train).astype('int')\n",
    "params['batch_size'] = 128\n",
    "params['logits_scale'] = 10\n",
    "params['logits_margin'] = 0.1\n",
    "params['feed_limit'] = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Model = LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 21:35:57.111392 140074959161088 deprecation.py:323] From <ipython-input-4-b715e0b7b635>:8: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0830 21:35:57.121677 140074959161088 deprecation.py:506] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0830 21:35:57.443397 140074959161088 deprecation.py:323] From <ipython-input-4-b715e0b7b635>:10: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0830 21:35:57.644582 140074959161088 deprecation.py:323] From <ipython-input-4-b715e0b7b635>:16: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0830 21:35:57.869934 140074959161088 deprecation.py:323] From <ipython-input-4-b715e0b7b635>:18: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(params['seed'])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_ohe = tf.placeholder(tf.float32, [None, params['n_classes']])\n",
    "quantized_y = tf.placeholder(tf.float32, [None, params['embedding']])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x, filters=6, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv1')\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=conv1, filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv2')\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "flatten = tf.layers.flatten(conv2)\n",
    "\n",
    "fc1 = tf.layers.dense(flatten, units=120, use_bias=True, activation=tf.nn.relu, name='fc1')\n",
    "fc2 = tf.layers.dense(fc1, units=84, use_bias=True, activation=tf.nn.relu, name='fc2')\n",
    "embedding_layer = tf.layers.dense(fc2, units=params['embedding'], use_bias=True, activation=tf.nn.sigmoid)\n",
    "\n",
    "clf_layer = tf.layers.dense(embedding_layer, units=params['n_classes'], activation=None, use_bias=False, name='clf_layer')\n",
    "softmax_layer = tf.nn.softmax(clf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1:(?, 14, 14, 6)\n",
      "conv2:(?, 5, 5, 16)\n",
      "fc1:(?, 120)\n",
      "fc2:(?, 84)\n",
      "embedding_layer:(?, 256)\n",
      "clf_layer:(?, 10)\n"
     ]
    }
   ],
   "source": [
    "print('conv1:{}'.format(conv1.shape))\n",
    "print('conv2:{}'.format(conv2.shape))\n",
    "print('fc1:{}'.format(fc1.shape))\n",
    "print('fc2:{}'.format(fc2.shape))\n",
    "print('embedding_layer:{}'.format(embedding_layer.shape))\n",
    "print('clf_layer:{}'.format(clf_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dhcq_loss(embedding_layer, clf_layer, y, quantized_y, params, gamma=1e-3):\n",
    "    cross_entropy = tf.reduce_sum(-y*tf.log(clf_layer) - (1-y)*tf.log(1-clf_layer)) / params['batch_size']\n",
    "    binary_diff_matrix = quantized_y - embedding_layer\n",
    "    quantization_loss = tf.trace(tf.matmul(tf.transpose(binary_diff_matrix), binary_diff_matrix)) * gamma\n",
    "    total_loss = cross_entropy + quantization_loss\n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-4\n",
    "epsilon = 1e-5\n",
    "loss = tf_dhcq_loss(embedding_layer=embedding_layer, clf_layer=softmax_layer, y=y_ohe, quantized_y=quantized_y, params=params, gamma=1e-5)\n",
    "loss += tf.losses.get_regularization_loss()\n",
    "train_model = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_code(x):\n",
    "    x[x >= 0.5] = 1\n",
    "    x[x < 0.5] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1    25.03sec \n",
      "           train(loss:279.7110, silhouette:-0.0498) \n",
      "           test (loss:278.8896, silhouette:-0.0525)\n",
      "\n",
      "epoch:2    24.06sec \n",
      "           train(loss:273.1373, silhouette:-0.0499) \n",
      "           test (loss:272.4274, silhouette:-0.0526)\n",
      "\n",
      "epoch:3    24.38sec \n",
      "           train(loss:268.6442, silhouette:-0.0501) \n",
      "           test (loss:268.0502, silhouette:-0.0528)\n",
      "\n",
      "epoch:4    24.49sec \n",
      "           train(loss:265.6192, silhouette:-0.0502) \n",
      "           test (loss:265.1340, silhouette:-0.0529)\n",
      "\n",
      "epoch:5    24.34sec \n",
      "           train(loss:263.6215, silhouette:-0.0504) \n",
      "           test (loss:263.2330, silhouette:-0.0531)\n",
      "\n",
      "epoch:6    24.38sec \n",
      "           train(loss:262.3283, silhouette:-0.0505) \n",
      "           test (loss:262.0219, silhouette:-0.0533)\n",
      "\n",
      "epoch:7    25.52sec \n",
      "           train(loss:261.5038, silhouette:-0.0507) \n",
      "           test (loss:261.2646, silhouette:-0.0535)\n",
      "\n",
      "epoch:8    24.46sec \n",
      "           train(loss:260.9841, silhouette:-0.0509) \n",
      "           test (loss:260.7986, silhouette:-0.0536)\n",
      "\n",
      "epoch:9    25.73sec \n",
      "           train(loss:260.6582, silhouette:-0.0510) \n",
      "           test (loss:260.5159, silhouette:-0.0538)\n",
      "\n",
      "epoch:10    24.83sec \n",
      "           train(loss:260.4540, silhouette:-0.0512) \n",
      "           test (loss:260.3453, silhouette:-0.0540)\n",
      "\n",
      "epoch:11    25.78sec \n",
      "           train(loss:260.3250, silhouette:-0.0513) \n",
      "           test (loss:260.2423, silhouette:-0.0542)\n",
      "\n",
      "epoch:12    25.61sec \n",
      "           train(loss:260.2419, silhouette:-0.0515) \n",
      "           test (loss:260.1797, silhouette:-0.0544)\n",
      "\n",
      "epoch:13    25.78sec \n",
      "           train(loss:260.1877, silhouette:-0.0516) \n",
      "           test (loss:260.1409, silhouette:-0.0546)\n",
      "\n",
      "epoch:14    25.53sec \n",
      "           train(loss:260.1513, silhouette:-0.0518) \n",
      "           test (loss:260.1162, silhouette:-0.0548)\n",
      "\n",
      "epoch:15    26.52sec \n",
      "           train(loss:260.1254, silhouette:-0.0520) \n",
      "           test (loss:260.0995, silhouette:-0.0551)\n",
      "\n",
      "epoch:16    32.88sec \n",
      "           train(loss:260.1064, silhouette:-0.0522) \n",
      "           test (loss:260.0873, silhouette:-0.0553)\n",
      "\n",
      "epoch:17    28.49sec \n",
      "           train(loss:260.0920, silhouette:-0.0524) \n",
      "           test (loss:260.0779, silhouette:-0.0555)\n",
      "\n",
      "epoch:18    35.92sec \n",
      "           train(loss:260.0793, silhouette:-0.0526) \n",
      "           test (loss:260.0696, silhouette:-0.0558)\n",
      "\n",
      "epoch:19    29.18sec \n",
      "           train(loss:260.0693, silhouette:-0.0528) \n",
      "           test (loss:260.0624, silhouette:-0.0560)\n",
      "\n",
      "epoch:20    38.54sec \n",
      "           train(loss:260.0600, silhouette:-0.0530) \n",
      "           test (loss:260.0555, silhouette:-0.0563)\n",
      "\n",
      "epoch:21    41.71sec \n",
      "           train(loss:260.0519, silhouette:-0.0532) \n",
      "           test (loss:260.0488, silhouette:-0.0565)\n",
      "\n",
      "epoch:22    44.30sec \n",
      "           train(loss:260.0435, silhouette:-0.0534) \n",
      "           test (loss:260.0421, silhouette:-0.0568)\n",
      "\n",
      "epoch:23    40.79sec \n",
      "           train(loss:260.0357, silhouette:-0.0536) \n",
      "           test (loss:260.0354, silhouette:-0.0570)\n",
      "\n",
      "epoch:24    41.39sec \n",
      "           train(loss:260.0282, silhouette:-0.0538) \n",
      "           test (loss:260.0287, silhouette:-0.0573)\n",
      "\n",
      "epoch:25    34.13sec \n",
      "           train(loss:260.0206, silhouette:-0.0540) \n",
      "           test (loss:260.0218, silhouette:-0.0576)\n",
      "\n",
      "epoch:26    23.52sec \n",
      "           train(loss:260.0130, silhouette:-0.0543) \n",
      "           test (loss:260.0148, silhouette:-0.0579)\n",
      "\n",
      "epoch:27    23.88sec \n",
      "           train(loss:260.0054, silhouette:-0.0545) \n",
      "           test (loss:260.0076, silhouette:-0.0582)\n",
      "\n",
      "epoch:28    23.48sec \n",
      "           train(loss:259.9978, silhouette:-0.0548) \n",
      "           test (loss:260.0003, silhouette:-0.0585)\n",
      "\n",
      "epoch:29    24.25sec \n",
      "           train(loss:259.9900, silhouette:-0.0550) \n",
      "           test (loss:259.9928, silhouette:-0.0588)\n",
      "\n",
      "epoch:30    23.83sec \n",
      "           train(loss:259.9824, silhouette:-0.0553) \n",
      "           test (loss:259.9852, silhouette:-0.0591)\n",
      "\n",
      "epoch:31    23.51sec \n",
      "           train(loss:259.9745, silhouette:-0.0555) \n",
      "           test (loss:259.9774, silhouette:-0.0594)\n",
      "\n",
      "epoch:32    23.81sec \n",
      "           train(loss:259.9668, silhouette:-0.0558) \n",
      "           test (loss:259.9695, silhouette:-0.0597)\n",
      "\n",
      "epoch:33    23.47sec \n",
      "           train(loss:259.9585, silhouette:-0.0561) \n",
      "           test (loss:259.9613, silhouette:-0.0600)\n",
      "\n",
      "epoch:34    23.55sec \n",
      "           train(loss:259.9501, silhouette:-0.0563) \n",
      "           test (loss:259.9529, silhouette:-0.0603)\n",
      "\n",
      "epoch:35    23.66sec \n",
      "           train(loss:259.9415, silhouette:-0.0566) \n",
      "           test (loss:259.9444, silhouette:-0.0607)\n",
      "\n",
      "epoch:36    24.14sec \n",
      "           train(loss:259.9323, silhouette:-0.0569) \n",
      "           test (loss:259.9357, silhouette:-0.0610)\n",
      "\n",
      "epoch:37    24.15sec \n",
      "           train(loss:259.9231, silhouette:-0.0572) \n",
      "           test (loss:259.9267, silhouette:-0.0613)\n",
      "\n",
      "epoch:38    23.64sec \n",
      "           train(loss:259.9142, silhouette:-0.0575) \n",
      "           test (loss:259.9176, silhouette:-0.0617)\n",
      "\n",
      "epoch:39    23.83sec \n",
      "           train(loss:259.9048, silhouette:-0.0579) \n",
      "           test (loss:259.9083, silhouette:-0.0620)\n",
      "\n",
      "epoch:40    24.25sec \n",
      "           train(loss:259.8951, silhouette:-0.0582) \n",
      "           test (loss:259.8988, silhouette:-0.0624)\n",
      "\n",
      "epoch:41    24.13sec \n",
      "           train(loss:259.8855, silhouette:-0.0585) \n",
      "           test (loss:259.8892, silhouette:-0.0627)\n",
      "\n",
      "epoch:42    23.91sec \n",
      "           train(loss:259.8757, silhouette:-0.0588) \n",
      "           test (loss:259.8793, silhouette:-0.0631)\n",
      "\n",
      "epoch:43    24.02sec \n",
      "           train(loss:259.8659, silhouette:-0.0592) \n",
      "           test (loss:259.8692, silhouette:-0.0634)\n",
      "\n",
      "epoch:44    23.75sec \n",
      "           train(loss:259.8557, silhouette:-0.0595) \n",
      "           test (loss:259.8589, silhouette:-0.0638)\n",
      "\n",
      "epoch:45    23.72sec \n",
      "           train(loss:259.8449, silhouette:-0.0599) \n",
      "           test (loss:259.8485, silhouette:-0.0642)\n",
      "\n",
      "epoch:46    24.66sec \n",
      "           train(loss:259.8340, silhouette:-0.0602) \n",
      "           test (loss:259.8377, silhouette:-0.0645)\n",
      "\n",
      "epoch:47    23.87sec \n",
      "           train(loss:259.8230, silhouette:-0.0606) \n",
      "           test (loss:259.8268, silhouette:-0.0649)\n",
      "\n",
      "epoch:48    23.80sec \n",
      "           train(loss:259.8116, silhouette:-0.0610) \n",
      "           test (loss:259.8156, silhouette:-0.0653)\n",
      "\n",
      "epoch:49    23.80sec \n",
      "           train(loss:259.7999, silhouette:-0.0614) \n",
      "           test (loss:259.8042, silhouette:-0.0656)\n",
      "\n",
      "epoch:50    24.06sec \n",
      "           train(loss:259.7877, silhouette:-0.0618) \n",
      "           test (loss:259.7925, silhouette:-0.0660)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_path_train = []\n",
    "loss_path_test = []\n",
    "\n",
    "embedding_train = []\n",
    "embedding_test = []\n",
    "\n",
    "silhouette_train = []\n",
    "silhouette_test = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "iter_cnt = 50\n",
    "np.random.seed(params['seed'])\n",
    "sample_size = 10000 # use 10000 samples only for monitoring\n",
    "\n",
    "# initial embedding train/test \n",
    "embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]})) \n",
    "\n",
    "for epoch in range(iter_cnt):\n",
    "    start_time = time.time()\n",
    "    cursor = 0\n",
    "    step = 1\n",
    "    \n",
    "    # random shuffle\n",
    "    train_idx = np.arange(len(y_train))\n",
    "    np.random.shuffle(train_idx)\n",
    "    shuffled_x_train = x_train[train_idx]\n",
    "    shuffled_y_train_ohe = y_train_ohe[train_idx]\n",
    "\n",
    "    while cursor < len(y_train): \n",
    "        batch_x_train = shuffled_x_train[cursor:cursor+params['batch_size']]\n",
    "        batch_y_train_ohe = shuffled_y_train_ohe[cursor:cursor+params['batch_size']] \n",
    "        sess.run(train_model, feed_dict={x:batch_x_train, y_ohe:batch_y_train_ohe,\n",
    "                                         quantized_y:generate_binary_code(sess.run(embedding_layer, feed_dict={x:shuffled_x_train[cursor:cursor+params['batch_size']]}))})       \n",
    "        step += 1\n",
    "        cursor += params['batch_size']\n",
    "    \n",
    "    # embedding train/test\n",
    "    embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "    embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]}))\n",
    "    \n",
    "    # silhouette train/query\n",
    "    silhouette_train.append(silhouette_score(embedding_train[-1], y_train[:sample_size]))\n",
    "    silhouette_test.append(silhouette_score(embedding_test[-1], y_test[:sample_size]))    \n",
    "    \n",
    "    # loss train/test\n",
    "    loss_path_train.append(sess.run(loss, feed_dict={x:x_train[:sample_size], y_ohe:y_train_ohe[:sample_size], quantized_y:generate_binary_code(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))}))\n",
    "    loss_path_test.append(sess.run(loss, feed_dict={x:x_test[:sample_size], y_ohe:y_test_ohe[:sample_size], quantized_y:generate_binary_code(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]}))}))\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('epoch:{}    {:.2f}sec \\n\\\n",
    "           train(loss:{:.4f}, silhouette:{:.4f}) \\n\\\n",
    "           test (loss:{:.4f}, silhouette:{:.4f})'.format(\n",
    "           epoch+1, end_time - start_time,\n",
    "           loss_path_train[-1], silhouette_train[-1],\n",
    "           loss_path_test[-1], silhouette_test[-1]))\n",
    "    print('')\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
