{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:(50000, 32, 32, 3), y_train:(50000,)\n",
      "x_test:(10000, 32, 32, 3), y_test:(10000,)\n",
      "y_train_ohe: (50000, 10)\n",
      "y_test_ohe: (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "ohe = OneHotEncoder()\n",
    "y_train_ohe = ohe.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "y_test_ohe = ohe.transform(y_test.reshape(-1,1)).toarray().astype('float32')\n",
    "print('x_train:{}, y_train:{}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test:{}, y_test:{}'.format(x_test.shape, y_test.shape))\n",
    "print('y_train_ohe:', y_train_ohe.shape)\n",
    "print('y_test_ohe:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['seed'] = 0\n",
    "params['embedding'] = 256\n",
    "params['n_classes'] = 10\n",
    "params['labels'] = np.unique(y_train).astype('int')\n",
    "params['batch_size'] = 128\n",
    "params['logits_scale'] = 10\n",
    "params['logits_margin'] = 0.1\n",
    "params['feed_limit'] = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Model = LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 20:13:30.452948 139864449722112 deprecation.py:323] From <ipython-input-4-820e5c110086>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0830 20:13:30.458902 139864449722112 deprecation.py:506] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0830 20:13:30.730068 139864449722112 deprecation.py:323] From <ipython-input-4-820e5c110086>:9: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0830 20:13:30.876702 139864449722112 deprecation.py:323] From <ipython-input-4-820e5c110086>:15: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0830 20:13:31.096442 139864449722112 deprecation.py:323] From <ipython-input-4-820e5c110086>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(params['seed'])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_ohe = tf.placeholder(tf.float32, [None, params['n_classes']])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x, filters=6, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv1')\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=conv1, filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv2')\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "flatten = tf.layers.flatten(conv2)\n",
    "\n",
    "fc1 = tf.layers.dense(flatten, units=120, use_bias=True, activation=tf.nn.relu, name='fc1')\n",
    "fc2 = tf.layers.dense(fc1, units=84, use_bias=True, activation=tf.nn.relu, name='fc2')\n",
    "embedding_layer = tf.layers.dense(fc2, units=params['embedding'], use_bias=True, activation=tf.nn.sigmoid)\n",
    "\n",
    "clf_layer = tf.layers.dense(embedding_layer, units=params['n_classes'], activation=None, use_bias=False, name='clf_layer')\n",
    "softmax_layer = tf.nn.softmax(clf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1:(?, 14, 14, 6)\n",
      "conv2:(?, 5, 5, 16)\n",
      "fc1:(?, 120)\n",
      "fc2:(?, 84)\n",
      "embedding_layer:(?, 256)\n",
      "clf_layer:(?, 10)\n"
     ]
    }
   ],
   "source": [
    "print('conv1:{}'.format(conv1.shape))\n",
    "print('conv2:{}'.format(conv2.shape))\n",
    "print('fc1:{}'.format(fc1.shape))\n",
    "print('fc2:{}'.format(fc2.shape))\n",
    "print('embedding_layer:{}'.format(embedding_layer.shape))\n",
    "print('clf_layer:{}'.format(clf_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 20:13:31.577783 139864449722112 deprecation.py:323] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "eta = 1e-4\n",
    "epsilon = 1e-5\n",
    "loss = tf.reduce_sum(tf.maximum(tf.multiply(-tf.log(softmax_layer + epsilon), y_ohe), 0))\n",
    "train_model = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1    15.96sec \n",
      "           train(loss:23003.7070, silhouette:-0.0503) \n",
      "           test (loss:23002.3477, silhouette:-0.0530)\n",
      "\n",
      "epoch:2    16.38sec \n",
      "           train(loss:22912.2559, silhouette:-0.0607) \n",
      "           test (loss:22906.1133, silhouette:-0.0615)\n",
      "\n",
      "epoch:3    15.93sec \n",
      "           train(loss:22226.2891, silhouette:-0.0852) \n",
      "           test (loss:22203.1562, silhouette:-0.0819)\n",
      "\n",
      "epoch:4    15.87sec \n",
      "           train(loss:21323.6484, silhouette:-0.0930) \n",
      "           test (loss:21350.4688, silhouette:-0.0880)\n",
      "\n",
      "epoch:5    16.16sec \n",
      "           train(loss:20036.7500, silhouette:-0.0875) \n",
      "           test (loss:20097.5234, silhouette:-0.0861)\n",
      "\n",
      "epoch:6    16.32sec \n",
      "           train(loss:18923.8789, silhouette:-0.0895) \n",
      "           test (loss:19010.1895, silhouette:-0.0895)\n",
      "\n",
      "epoch:7    16.03sec \n",
      "           train(loss:18023.0977, silhouette:-0.0758) \n",
      "           test (loss:18076.6719, silhouette:-0.0777)\n",
      "\n",
      "epoch:8    16.55sec \n",
      "           train(loss:17173.1172, silhouette:-0.0631) \n",
      "           test (loss:17239.8789, silhouette:-0.0653)\n",
      "\n",
      "epoch:9    16.09sec \n",
      "           train(loss:16298.2617, silhouette:-0.0521) \n",
      "           test (loss:16431.2500, silhouette:-0.0542)\n",
      "\n",
      "epoch:10    16.49sec \n",
      "           train(loss:15796.6553, silhouette:-0.0465) \n",
      "           test (loss:15955.8672, silhouette:-0.0486)\n",
      "\n",
      "epoch:11    16.39sec \n",
      "           train(loss:15703.7793, silhouette:-0.0397) \n",
      "           test (loss:15797.6377, silhouette:-0.0413)\n",
      "\n",
      "epoch:12    17.37sec \n",
      "           train(loss:15033.1201, silhouette:-0.0352) \n",
      "           test (loss:15183.6553, silhouette:-0.0372)\n",
      "\n",
      "epoch:13    17.36sec \n",
      "           train(loss:14802.1572, silhouette:-0.0306) \n",
      "           test (loss:14951.4629, silhouette:-0.0326)\n",
      "\n",
      "epoch:14    17.60sec \n",
      "           train(loss:14738.2441, silhouette:-0.0288) \n",
      "           test (loss:14935.9775, silhouette:-0.0305)\n",
      "\n",
      "epoch:15    16.88sec \n",
      "           train(loss:14287.2471, silhouette:-0.0252) \n",
      "           test (loss:14510.2637, silhouette:-0.0271)\n",
      "\n",
      "epoch:16    17.31sec \n",
      "           train(loss:14260.3955, silhouette:-0.0249) \n",
      "           test (loss:14483.0156, silhouette:-0.0270)\n",
      "\n",
      "epoch:17    17.03sec \n",
      "           train(loss:14061.5312, silhouette:-0.0221) \n",
      "           test (loss:14301.2744, silhouette:-0.0242)\n",
      "\n",
      "epoch:18    17.82sec \n",
      "           train(loss:13999.9355, silhouette:-0.0225) \n",
      "           test (loss:14255.8555, silhouette:-0.0249)\n",
      "\n",
      "epoch:19    17.24sec \n",
      "           train(loss:13789.9062, silhouette:-0.0193) \n",
      "           test (loss:14058.1934, silhouette:-0.0219)\n",
      "\n",
      "epoch:20    17.86sec \n",
      "           train(loss:13478.3594, silhouette:-0.0166) \n",
      "           test (loss:13834.6465, silhouette:-0.0192)\n",
      "\n",
      "epoch:21    17.28sec \n",
      "           train(loss:13344.7402, silhouette:-0.0148) \n",
      "           test (loss:13710.1084, silhouette:-0.0176)\n",
      "\n",
      "epoch:22    18.08sec \n",
      "           train(loss:13203.9375, silhouette:-0.0130) \n",
      "           test (loss:13633.0430, silhouette:-0.0160)\n",
      "\n",
      "epoch:23    17.92sec \n",
      "           train(loss:13230.8135, silhouette:-0.0122) \n",
      "           test (loss:13621.7715, silhouette:-0.0152)\n",
      "\n",
      "epoch:24    17.92sec \n",
      "           train(loss:13009.3691, silhouette:-0.0108) \n",
      "           test (loss:13457.1484, silhouette:-0.0139)\n",
      "\n",
      "epoch:25    17.45sec \n",
      "           train(loss:12937.6172, silhouette:-0.0096) \n",
      "           test (loss:13445.3057, silhouette:-0.0132)\n",
      "\n",
      "epoch:26    17.92sec \n",
      "           train(loss:12778.5498, silhouette:-0.0082) \n",
      "           test (loss:13324.9590, silhouette:-0.0118)\n",
      "\n",
      "epoch:27    17.47sec \n",
      "           train(loss:12758.4219, silhouette:-0.0076) \n",
      "           test (loss:13321.2480, silhouette:-0.0113)\n",
      "\n",
      "epoch:28    17.46sec \n",
      "           train(loss:12657.9639, silhouette:-0.0073) \n",
      "           test (loss:13242.7939, silhouette:-0.0111)\n",
      "\n",
      "epoch:29    17.72sec \n",
      "           train(loss:12441.4619, silhouette:-0.0057) \n",
      "           test (loss:13074.7803, silhouette:-0.0098)\n",
      "\n",
      "epoch:30    17.86sec \n",
      "           train(loss:12369.2344, silhouette:-0.0049) \n",
      "           test (loss:13044.8223, silhouette:-0.0093)\n",
      "\n",
      "epoch:31    18.01sec \n",
      "           train(loss:12257.6357, silhouette:-0.0040) \n",
      "           test (loss:12942.8066, silhouette:-0.0083)\n",
      "\n",
      "epoch:32    18.48sec \n",
      "           train(loss:12436.9404, silhouette:-0.0036) \n",
      "           test (loss:13128.1152, silhouette:-0.0080)\n",
      "\n",
      "epoch:33    19.10sec \n",
      "           train(loss:12182.9961, silhouette:-0.0027) \n",
      "           test (loss:12983.6191, silhouette:-0.0073)\n",
      "\n",
      "epoch:34    18.36sec \n",
      "           train(loss:11939.0449, silhouette:-0.0016) \n",
      "           test (loss:12713.3643, silhouette:-0.0067)\n",
      "\n",
      "epoch:35    17.80sec \n",
      "           train(loss:11908.0176, silhouette:-0.0009) \n",
      "           test (loss:12735.7910, silhouette:-0.0059)\n",
      "\n",
      "epoch:36    18.33sec \n",
      "           train(loss:11726.3701, silhouette:0.0010) \n",
      "           test (loss:12604.1387, silhouette:-0.0043)\n",
      "\n",
      "epoch:37    17.74sec \n",
      "           train(loss:12221.9902, silhouette:-0.0008) \n",
      "           test (loss:13103.2500, silhouette:-0.0064)\n",
      "\n",
      "epoch:38    18.42sec \n",
      "           train(loss:11639.1543, silhouette:0.0023) \n",
      "           test (loss:12552.5527, silhouette:-0.0035)\n",
      "\n",
      "epoch:39    18.18sec \n",
      "           train(loss:11509.0215, silhouette:0.0029) \n",
      "           test (loss:12506.3633, silhouette:-0.0029)\n",
      "\n",
      "epoch:40    18.47sec \n",
      "           train(loss:11569.8613, silhouette:0.0029) \n",
      "           test (loss:12526.2578, silhouette:-0.0033)\n",
      "\n",
      "epoch:41    18.31sec \n",
      "           train(loss:11499.6133, silhouette:0.0038) \n",
      "           test (loss:12530.6309, silhouette:-0.0023)\n",
      "\n",
      "epoch:42    19.51sec \n",
      "           train(loss:11410.8525, silhouette:0.0038) \n",
      "           test (loss:12504.0723, silhouette:-0.0027)\n",
      "\n",
      "epoch:43    18.66sec \n",
      "           train(loss:11277.5352, silhouette:0.0056) \n",
      "           test (loss:12339.1133, silhouette:-0.0008)\n",
      "\n",
      "epoch:44    18.80sec \n",
      "           train(loss:11085.6543, silhouette:0.0060) \n",
      "           test (loss:12193.5420, silhouette:-0.0009)\n",
      "\n",
      "epoch:45    17.95sec \n",
      "           train(loss:11156.7637, silhouette:0.0064) \n",
      "           test (loss:12307.5645, silhouette:-0.0003)\n",
      "\n",
      "epoch:46    18.34sec \n",
      "           train(loss:11133.6064, silhouette:0.0066) \n",
      "           test (loss:12289.4551, silhouette:-0.0005)\n",
      "\n",
      "epoch:47    18.19sec \n",
      "           train(loss:10845.0752, silhouette:0.0079) \n",
      "           test (loss:12079.6836, silhouette:0.0008)\n",
      "\n",
      "epoch:48    18.67sec \n",
      "           train(loss:10909.4746, silhouette:0.0086) \n",
      "           test (loss:12137.5957, silhouette:0.0012)\n",
      "\n",
      "epoch:49    18.42sec \n",
      "           train(loss:11000.5293, silhouette:0.0081) \n",
      "           test (loss:12281.6797, silhouette:0.0006)\n",
      "\n",
      "epoch:50    18.63sec \n",
      "           train(loss:10794.5557, silhouette:0.0097) \n",
      "           test (loss:12130.8135, silhouette:0.0017)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_path_train = []\n",
    "loss_path_test = []\n",
    "\n",
    "embedding_train = []\n",
    "embedding_test = []\n",
    "\n",
    "silhouette_train = []\n",
    "silhouette_test = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "iter_cnt = 50\n",
    "np.random.seed(params['seed'])\n",
    "sample_size = 10000 # use 10000 samples only for monitoring\n",
    "\n",
    "# initial embedding train/test \n",
    "embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]})) \n",
    "\n",
    "for epoch in range(iter_cnt):\n",
    "    start_time = time.time()\n",
    "    cursor = 0\n",
    "    step = 1\n",
    "    \n",
    "    # random shuffle\n",
    "    train_idx = np.arange(len(y_train))\n",
    "    np.random.shuffle(train_idx)\n",
    "    shuffled_x_train = x_train[train_idx]\n",
    "    shuffled_y_train_ohe = y_train_ohe[train_idx]\n",
    "\n",
    "    while cursor < len(y_train): \n",
    "        batch_x_train = shuffled_x_train[cursor:cursor+params['batch_size']]\n",
    "        batch_y_train_ohe = shuffled_y_train_ohe[cursor:cursor+params['batch_size']] \n",
    "        sess.run(train_model, feed_dict={x:batch_x_train, y_ohe:batch_y_train_ohe})       \n",
    "        step += 1\n",
    "        cursor += params['batch_size']\n",
    "    \n",
    "    # embedding train/test\n",
    "    embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "    embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]}))\n",
    "    \n",
    "    # silhouette train/query\n",
    "    silhouette_train.append(silhouette_score(embedding_train[-1], y_train[:sample_size]))\n",
    "    silhouette_test.append(silhouette_score(embedding_test[-1], y_test[:sample_size]))    \n",
    "    \n",
    "    # loss train/test\n",
    "    loss_path_train.append(sess.run(loss, feed_dict={x:x_train[:sample_size], y_ohe:y_train_ohe[:sample_size]}))\n",
    "    loss_path_test.append(sess.run(loss, feed_dict={x:x_test[:sample_size], y_ohe:y_test_ohe[:sample_size]}))\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('epoch:{}    {:.2f}sec \\n\\\n",
    "           train(loss:{:.4f}, silhouette:{:.4f}) \\n\\\n",
    "           test (loss:{:.4f}, silhouette:{:.4f})'.format(\n",
    "           epoch+1, end_time - start_time,\n",
    "           loss_path_train[-1], silhouette_train[-1],\n",
    "           loss_path_test[-1], silhouette_test[-1]))\n",
    "    print('')\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
