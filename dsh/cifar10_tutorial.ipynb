{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:(50000, 32, 32, 3), y_train:(50000,)\n",
      "x_test:(10000, 32, 32, 3), y_test:(10000,)\n",
      "y_train_ohe: (50000, 10)\n",
      "y_test_ohe: (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "ohe = OneHotEncoder()\n",
    "y_train_ohe = ohe.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "y_test_ohe = ohe.transform(y_test.reshape(-1,1)).toarray().astype('float32')\n",
    "print('x_train:{}, y_train:{}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test:{}, y_test:{}'.format(x_test.shape, y_test.shape))\n",
    "print('y_train_ohe:', y_train_ohe.shape)\n",
    "print('y_test_ohe:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['seed'] = 0\n",
    "params['embedding'] = 256\n",
    "params['n_classes'] = 10\n",
    "params['labels'] = np.unique(y_train).astype('int')\n",
    "params['batch_size'] = 128\n",
    "params['logits_scale'] = 10\n",
    "params['logits_margin'] = 0.05\n",
    "params['feed_limit'] = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Model = LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 02:13:22.394888 140665120442112 deprecation.py:323] From <ipython-input-4-ae8ed2ac1335>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0831 02:13:22.403779 140665120442112 deprecation.py:506] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0831 02:13:22.738742 140665120442112 deprecation.py:323] From <ipython-input-4-ae8ed2ac1335>:9: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0831 02:13:22.960643 140665120442112 deprecation.py:323] From <ipython-input-4-ae8ed2ac1335>:15: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0831 02:13:23.325620 140665120442112 deprecation.py:323] From <ipython-input-4-ae8ed2ac1335>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(params['seed'])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, [None, ])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x, filters=6, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv1')\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=conv1, filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv2')\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "flatten = tf.layers.flatten(conv2)\n",
    "\n",
    "fc1 = tf.layers.dense(flatten, units=120, use_bias=True, activation=tf.nn.relu, name='fc1')\n",
    "fc2 = tf.layers.dense(fc1, units=84, use_bias=True, activation=tf.nn.relu, name='fc2')\n",
    "embedding_layer = tf.layers.dense(fc2, units=params['embedding'], use_bias=True, activation=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1:(?, 14, 14, 6)\n",
      "conv2:(?, 5, 5, 16)\n",
      "fc1:(?, 120)\n",
      "fc2:(?, 84)\n",
      "embedding_layer:(?, 256)\n"
     ]
    }
   ],
   "source": [
    "print('conv1:{}'.format(conv1.shape))\n",
    "print('conv2:{}'.format(conv2.shape))\n",
    "print('fc1:{}'.format(fc1.shape))\n",
    "print('fc2:{}'.format(fc2.shape))\n",
    "print('embedding_layer:{}'.format(embedding_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embedding_layer, y, params, margin, alpha=1e-2):\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    loss_list = []\n",
    "    \n",
    "    for i in np.arange(params['batch_size']):\n",
    "        label = y[i]\n",
    "        positive_mask = tf.equal(y, label)\n",
    "        positive_embedding = tf.boolean_mask(embedding_layer, positive_mask)\n",
    "        positive_loss = 0.5 * tf.reduce_sum(tf.square(embedding_layer[i] - positive_embedding), axis=1)\n",
    "        positive_regularizer = alpha * (tf.reduce_sum(tf.abs(tf.abs(embedding_layer[i]) - 1)) + tf.reduce_sum(tf.abs(tf.abs(positive_embedding) - 1), axis=1))\n",
    "        positive_loss += positive_regularizer\n",
    "         \n",
    "        negative_mask = tf.not_equal(y, label)\n",
    "        negative_cnt = tf.cast(tf.count_nonzero(negative_mask), tf.int32)\n",
    "        negative_embedding = tf.boolean_mask(embedding_layer, negative_mask)\n",
    "        negative_loss = 0.5 * tf.maximum((margin - tf.reduce_sum(tf.square(embedding_layer[i] - negative_embedding), axis=1)), \n",
    "                                         tf.zeros(shape=[negative_cnt, ]))\n",
    "        negative_regularizer = alpha * (tf.reduce_sum(tf.abs(tf.abs(embedding_layer[i]) - 1)) + tf.reduce_sum(tf.abs(tf.abs(negative_embedding) - 1), axis=1))\n",
    "        negative_loss += negative_regularizer\n",
    "        loss_sum = tf.reduce_sum(positive_loss) + tf.reduce_sum(negative_loss)\n",
    "        loss_list.append(loss_sum)\n",
    "    total_loss = tf.reduce_sum(loss_list)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0831 02:13:24.137611 140665120442112 deprecation.py:323] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "eta = 1e-3\n",
    "epsilon = 1e-5\n",
    "loss = loss = contrastive_loss(embedding_layer=embedding_layer, y=y, params=params, margin=params['embedding']*2, alpha=1e-3)\n",
    "train_model = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1    188.84sec \n",
      "           train(loss:295202752.0000, silhouette:-0.8023) \n",
      "           test (loss:295201792.0000, silhouette:-0.7972)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_path_train = []\n",
    "loss_path_test = []\n",
    "\n",
    "embedding_train = []\n",
    "embedding_test = []\n",
    "\n",
    "silhouette_train = []\n",
    "silhouette_test = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "iter_cnt = 50\n",
    "np.random.seed(params['seed'])\n",
    "sample_size = 10000 # use 10000 samples only for monitoring\n",
    "\n",
    "# initial embedding train/test \n",
    "embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]})) \n",
    "\n",
    "for epoch in range(iter_cnt):\n",
    "    start_time = time.time()\n",
    "    cursor = 0\n",
    "    step = 1\n",
    "    \n",
    "    # random shuffle\n",
    "    train_idx = np.arange(len(y_train))\n",
    "    np.random.shuffle(train_idx)\n",
    "    shuffled_x_train = x_train[train_idx]\n",
    "    shuffled_y_train = y_train[train_idx]\n",
    "\n",
    "    while cursor + params['batch_size'] < len(y_train): \n",
    "        batch_x_train = shuffled_x_train[cursor:cursor+params['batch_size']]\n",
    "        batch_y_train = shuffled_y_train[cursor:cursor+params['batch_size']] \n",
    "        sess.run(train_model, feed_dict={x:batch_x_train, y:batch_y_train})       \n",
    "        step += 1\n",
    "        cursor += params['batch_size']\n",
    "    \n",
    "    # embedding train/test\n",
    "    embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "    embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]}))\n",
    "    \n",
    "    # silhouette train/query\n",
    "    silhouette_train.append(silhouette_score(embedding_train[-1], y_train[:sample_size]))\n",
    "    silhouette_test.append(silhouette_score(embedding_test[-1], y_test[:sample_size]))    \n",
    "    \n",
    "    # loss train/test\n",
    "    loss_path_train.append(sess.run(loss, feed_dict={x:x_train[:sample_size], y:y_train[:sample_size]}))\n",
    "    loss_path_test.append(sess.run(loss, feed_dict={x:x_test[:sample_size], y:y_test[:sample_size]}))\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('epoch:{}    {:.2f}sec \\n\\\n",
    "           train(loss:{:.4f}, silhouette:{:.4f}) \\n\\\n",
    "           test (loss:{:.4f}, silhouette:{:.4f})'.format(\n",
    "           epoch+1, end_time - start_time,\n",
    "           loss_path_train[-1], silhouette_train[-1],\n",
    "           loss_path_test[-1], silhouette_test[-1]))\n",
    "    print('')\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
